{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sin_wave = np.array([math.sin(x) for x in np.arange(200)])\n",
    "X = [] \n",
    "Y = []\n",
    "\n",
    "sequence_len = 50\n",
    "num_records = len(sin_wave) - sequence_len\n",
    "\n",
    "for i in range(num_records - 50):\n",
    "    X.append(sin_wave[i:i+sequence_len])\n",
    "    Y.append(sin_wave[i + sequence_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "X = np.expand_dims(X, axis=2)\n",
    "\n",
    "Y = np.array(X)\n",
    "Y = np.expand_dims(X, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "X_val = []\n",
    "Y_val = []\n",
    "\n",
    "for i in range(num_records - 50, num_records):\n",
    "    X_val.append(sin_wave[i:i + sequence_len])\n",
    "    Y_val.append(sin_wave[i + sequence_len])\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "X_val = np.expand_dims(X_val, axis=2)\n",
    "\n",
    "Y_val = np.array(Y_val)\n",
    "Y_val = np.expand_dims(Y_val, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0001    \n",
    "nepoch = 25               \n",
    "T = 50                   # length of sequence\n",
    "hidden_dim = 100         \n",
    "output_dim = 1\n",
    "\n",
    "bptt_truncate = 5\n",
    "min_clip_value = -10\n",
    "max_clip_value = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#U is the weight matrix for weights between input and hidden layers\n",
    "#V is the weight matrix for weights between hidden and output layers\n",
    "#W is the weight matrix for shared weights in the RNN layer (hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "U = np.random.uniform(0, 1, (hidden_dim, T))\n",
    "W = np.random.uniform(0, 1, (hidden_dim, hidden_dim))\n",
    "V = np.random.uniform(0, 1, (output_dim, hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step 2.1 : Check the loss on training data\n",
    "Step 2.1.1 : Forward Pass\n",
    "Step 2.1.2 : Calculate Error\n",
    "Step 2.2 : Check the loss on validation data\n",
    "Step 2.2.1 : Forward Pass\n",
    "Step 2.2.2 : Calculate Error\n",
    "Step 2.3 : Start actual training\n",
    "Step 2.3.1 : Forward Pass\n",
    "Step 2.3.2 : Backpropagate Error\n",
    "Step 2.3.3 : Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Epoch:  1 , Loss:  [[[127348.78984593]\n  [127374.47250441]\n  [127393.87453188]\n  [127389.63960128]\n  [127365.50552237]\n  [127343.30889145]\n  [127343.90605885]\n  [127366.72656432]\n  [127390.35830394]\n  [127393.45454148]\n  [127373.28322862]\n  [127347.91420518]\n  [127340.95283186]\n  [127359.04089644]\n  [127385.06609163]\n  [127395.26079183]\n  [127380.60124462]\n  [127354.11490771]\n  [127340.17890618]\n  [127352.03500046]\n  [127378.39993507]\n  [127394.92346206]\n  [127386.88866971]\n  [127361.39803229]\n  [127341.64971616]\n  [127346.28283506]\n  [127370.87386157]\n  [127392.4676954 ]\n  [127391.66308456]\n  [127369.16979571]\n  [127345.24099516]\n  [127342.26213972]\n  [127363.07824584]\n  [127388.07720535]\n  [127394.56322383]\n  [127376.80715018]\n  [127350.65089266]\n  [127340.31028064]\n  [127355.63563794]\n  [127382.08336105]\n  [127395.37180154]\n  [127383.70864962]\n  [127357.42894477]\n  [127340.59204242]\n  [127349.15068544]\n  [127374.9445289 ]\n  [127394.02852979]\n  [127389.34019285]\n  [127365.01805218]\n  [127343.08359694]]] , Val Loss:  [[63670.09291222]]\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8666ffb74ccd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mdV_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdmulv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m's'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mdsv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdmulv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (100,1) and (1,50,1) not aligned: 1 (dim 1) != 50 (dim 1)"
     ],
     "ename": "ValueError",
     "evalue": "shapes (100,1) and (1,50,1) not aligned: 1 (dim 1) != 50 (dim 1)",
     "output_type": "error"
    }
   ],
   "source": [
    "for epoch in range(nepoch):\n",
    "    # check loss on train\n",
    "    loss = 0.0\n",
    "    \n",
    "    # do a forward pass to get prediction\n",
    "    for i in range(Y.shape[0]):\n",
    "        x, y = X[i], Y[i]                    # get input, output values of each record\n",
    "        prev_s = np.zeros((hidden_dim, 1))   # here, prev-s is the value of the previous activation of hidden layer; which is initialized as all zeroes\n",
    "        for t in range(T):\n",
    "            new_input = np.zeros(x.shape)    # we then do a forward pass for every timestep in the sequence\n",
    "            new_input[t] = x[t]              # for this, we define a single input for that timestep\n",
    "            mulu = np.dot(U, new_input)\n",
    "            mulw = np.dot(W, prev_s)\n",
    "            add = mulw + mulu\n",
    "            s = sigmoid(add)\n",
    "            mulv = np.dot(V, s)\n",
    "            prev_s = s\n",
    "\n",
    "    # calculate error \n",
    "        loss_per_record = (y - mulv)**2 / 2\n",
    "        loss += loss_per_record\n",
    "    loss = loss / float(y.shape[0])\n",
    "    # check loss on val\n",
    "    val_loss = 0.0\n",
    "    for i in range(Y_val.shape[0]):\n",
    "        x, y = X_val[i], Y_val[i]\n",
    "        prev_s = np.zeros((hidden_dim, 1))\n",
    "        for t in range(T):\n",
    "            new_input = np.zeros(x.shape)\n",
    "            new_input[t] = x[t]\n",
    "            mulu = np.dot(U, new_input)\n",
    "            mulw = np.dot(W, prev_s)\n",
    "            add = mulw + mulu\n",
    "            s = sigmoid(add)\n",
    "            mulv = np.dot(V, s)\n",
    "            prev_s = s\n",
    "\n",
    "        loss_per_record = (y - mulv)**2 / 2\n",
    "        val_loss += loss_per_record\n",
    "    val_loss = val_loss / float(y.shape[0])\n",
    "\n",
    "    print('Epoch: ', epoch + 1, ', Loss: ', loss, ', Val Loss: ', val_loss)\n",
    "    # train model\n",
    "    for i in range(Y.shape[0]):\n",
    "        x, y = X[i], Y[i]\n",
    "    \n",
    "        layers = []\n",
    "        prev_s = np.zeros((hidden_dim, 1))\n",
    "        dU = np.zeros(U.shape)\n",
    "        dV = np.zeros(V.shape)\n",
    "        dW = np.zeros(W.shape)\n",
    "        \n",
    "        dU_t = np.zeros(U.shape)\n",
    "        dV_t = np.zeros(V.shape)\n",
    "        dW_t = np.zeros(W.shape)\n",
    "        \n",
    "        dU_i = np.zeros(U.shape)\n",
    "        dW_i = np.zeros(W.shape)\n",
    "        \n",
    "        # forward pass\n",
    "        for t in range(T):\n",
    "            new_input = np.zeros(x.shape)\n",
    "            new_input[t] = x[t]\n",
    "            mulu = np.dot(U, new_input)\n",
    "            mulw = np.dot(W, prev_s)\n",
    "            add = mulw + mulu\n",
    "            s = sigmoid(add)\n",
    "            mulv = np.dot(V, s)\n",
    "            layers.append({'s':s, 'prev_s':prev_s})\n",
    "            prev_s = s\n",
    "        # derivative of pred\n",
    "        dmulv = (mulv - y)\n",
    "        \n",
    "        # backward pass\n",
    "        for t in range(T):\n",
    "            dV_t = np.dot(dmulv, np.transpose(layers[t]['s']))\n",
    "            dsv = np.dot(np.transpose(V), dmulv)\n",
    "            \n",
    "            ds = dsv\n",
    "            dadd = add * (1 - add) * ds\n",
    "            \n",
    "            dmulw = dadd * np.ones_like(mulw)\n",
    "\n",
    "            dprev_s = np.dot(np.transpose(W), dmulw)\n",
    "\n",
    "\n",
    "            for i in range(t-1, max(-1, t-bptt_truncate-1), -1):\n",
    "                ds = dsv + dprev_s\n",
    "                dadd = add * (1 - add) * ds\n",
    "\n",
    "                dmulw = dadd * np.ones_like(mulw)\n",
    "                dmulu = dadd * np.ones_like(mulu)\n",
    "\n",
    "                dW_i = np.dot(W, layers[t]['prev_s'])\n",
    "                dprev_s = np.dot(np.transpose(W), dmulw)\n",
    "\n",
    "                new_input = np.zeros(x.shape)\n",
    "                new_input[t] = x[t]\n",
    "                dU_i = np.dot(U, new_input)\n",
    "                dx = np.dot(np.transpose(U), dmulu)\n",
    "\n",
    "                dU_t += dU_i\n",
    "                dW_t += dW_i\n",
    "                \n",
    "            dV += dV_t\n",
    "            dU += dU_t\n",
    "            dW += dW_t\n",
    "            if dU.max() > max_clip_value:\n",
    "                dU[dU > max_clip_value] = max_clip_value\n",
    "            if dV.max() > max_clip_value:\n",
    "                dV[dV > max_clip_value] = max_clip_value\n",
    "            if dW.max() > max_clip_value:\n",
    "                dW[dW > max_clip_value] = max_clip_value\n",
    "                \n",
    "            \n",
    "            if dU.min() < min_clip_value:\n",
    "                dU[dU < min_clip_value] = min_clip_value\n",
    "            if dV.min() < min_clip_value:\n",
    "                dV[dV < min_clip_value] = min_clip_value\n",
    "            if dW.min() < min_clip_value:\n",
    "                dW[dW < min_clip_value] = min_clip_value\n",
    "        \n",
    "        # update\n",
    "        U -= learning_rate * dU\n",
    "        V -= learning_rate * dV\n",
    "        W -= learning_rate * dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}